# -*- coding: utf-8 -*-
"""Copy_of_Copy_of_Neural_Receiver_IEEE80211p.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JMn7L5z9YyssFfh1BsYWy0rAgCMBDa2N
"""

# Import Sionna
try:
    import sionna
except ImportError as e:
    # Install Sionna if package is not already installed
    import os
    os.system("pip install sionna")
    import sionna

# import sionna
import os 
import sys
sys.path.insert(0, os.getcwd()+'/...') # +'/..'
sys.path.insert(0, os.getcwd()+'/..') # +'/..'
sys.path.insert(0, os.getcwd()+'/') # +'/..'

import tensorflow as tf
gpus = tf.config.list_physical_devices('GPU')
print('Number of GPUs available :', len(gpus))
if gpus:
    gpu_num = 0 # Index of the GPU to use
    try:
        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')
        print('Only GPU number', gpu_num, 'used.')
        tf.config.experimental.set_memory_growth(gpus[gpu_num], True)
    except RuntimeError as e:
        print(e)


# get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
import numpy as np
import pickle

from tensorflow.keras import Model
from tensorflow.keras.layers import Layer, Conv2D, LayerNormalization
from tensorflow.nn import relu

from sionna.mapping import Constellation
from sionna.channel.tr38901 import Antenna, AntennaArray, CDL
from sionna.channel import OFDMChannel
from sionna.mimo import StreamManagement
from sionna.ofdm import ResourceGrid, ResourceGridMapper, LSChannelEstimator, LMMSEEqualizer, RemoveNulledSubcarriers, ResourceGridDemapper
from sionna.utils import BinarySource, ebnodb2no, insert_dims, flatten_last_dims, log10, expand_to_rank
from sionna.fec.ldpc.encoding import LDPC5GEncoder
from sionna.fec.ldpc.decoding import LDPC5GDecoder
from sionna.fec.conv import ConvEncoder, ViterbiDecoder
from sionna.mapping import Mapper, Demapper
from sionna.utils.metrics import compute_ber
from sionna.utils import sim_ber
from sionna.ofdm import PilotPattern, RemoveNulledSubcarriers



import numpy as np
import matplotlib.pyplot as plt
import math
from scipy import special as sp
from constants import k48, k96, k192, k288
from constants import w48, w96, w192, w288

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import regularizers
from tensorflow.keras import optimizers
from tensorflow.keras import initializers
from tensorflow.keras.callbacks import TensorBoard
import pickle
from time import time
from datetime import datetime
from scipy.special import logsumexp




# from IEEE80211helper import generate_ieee8211_constellation
# from IEEE80211helper import IEEE80211Scrambler, IEEE80211Interleaver

# In[16]:


import math
import itertools

"""# 1. IEEE 802.11helper.py file"""

# IEEE 802.11helper.py file

# import numpy as np
# import tensorflow as tf



# from tensorflow.keras.layers import Layer



def generate_ieee8211_constellation(mu = 1, normalize = True, show_constellation = False):
    assert mu in (1,2,4,6),\
    "currently ony supports mu  = 1, 2, 4, and 6"
    if mu == 1:
        mapping_table = {
        (0,) : -1.+0.j,
        (1,) :  1.+0.j
        }
    elif mu == 2:
        mapping_table = {
            (0,0) : -1.-1.j,
            (0,1) : -1.+1.j,
            (1,0) :  1.-1.j,
            (1,1) :  1.+1.j
        }
    elif mu == 4:
        mapping_table = {
            (0,0,0,0) : -3.-3.j,
            (0,0,0,1) : -3.-1.j,
            (0,0,1,0) : -3.+3.j,
            (0,0,1,1) : -3.+1.j,
            (0,1,0,0) : -1.-3.j,
            (0,1,0,1) : -1.-1.j,
            (0,1,1,0) : -1.+3.j,
            (0,1,1,1) : -1.+1.j,
            (1,0,0,0) :  3.-3.j,
            (1,0,0,1) :  3.-1.j,
            (1,0,1,0) :  3.+3.j,
            (1,0,1,1) :  3.+1.j,
            (1,1,0,0) :  1.-3.j,
            (1,1,0,1) :  1.-1.j,
            (1,1,1,0) :  1.+3.j,
            (1,1,1,1) :  1.+1.j
        }
    elif mu == 6:
        mapping_table = {
            (0,0,0,0,0,0)   :  -7.-7.j, 
            (0,0,0,0,0,1)   :  -7.-5.j, 
            (0,0,0,0,1,0)   :  -7.-1.j, 
            (0,0,0,0,1,1)   :  -7.-3.j, 
            (0,0,0,1,0,0)   :  -7.+7.j, 
            (0,0,0,1,0,1)   :  -7.+5.j, 
            (0,0,0,1,1,0)   :  -7.+1.j, 
            (0,0,0,1,1,1)   :  -7.+3.j, 
            (0,0,1,0,0,0)   :  -5.-7.j, 
            (0,0,1,0,0,1)   :  -5.-5.j, 
            (0,0,1,0,1,0)   :  -5.-1.j, 
            (0,0,1,0,1,1)   :  -5.-3.j, 
            (0,0,1,1,0,0)   :  -5.+7.j, 
            (0,0,1,1,0,1)   :  -5.+5.j, 
            (0,0,1,1,1,0)   :  -5.+1.j, 
            (0,0,1,1,1,1)   :  -5.+3.j, 
            (0,1,0,0,0,0)   :  -1.-7.j, 
            (0,1,0,0,0,1)   :  -1.-5.j, 
            (0,1,0,0,1,0)   :  -1.-1.j, 
            (0,1,0,0,1,1)   :  -1.-3.j, 
            (0,1,0,1,0,0)   :  -1.+7.j, 
            (0,1,0,1,0,1)   :  -1.+5.j, 
            (0,1,0,1,1,0)   :  -1.+1.j, 
            (0,1,0,1,1,1)   :  -1.+3.j, 
            (0,1,1,0,0,0)   :  -3.-7.j, 
            (0,1,1,0,0,1)   :  -3.-5.j, 
            (0,1,1,0,1,0)   :  -3.-1.j, 
            (0,1,1,0,1,1)   :  -3.-3.j, 
            (0,1,1,1,0,0)   :  -3.+7.j, 
            (0,1,1,1,0,1)   :  -3.+5.j, 
            (0,1,1,1,1,0)   :  -3.+1.j, 
            (0,1,1,1,1,1)   :  -3.+3.j, 
            (1,0,0,0,0,0)   :   7.-7.j, 
            (1,0,0,0,0,1)   :   7.-5.j, 
            (1,0,0,0,1,0)   :   7.-1.j, 
            (1,0,0,0,1,1)   :   7.-3.j, 
            (1,0,0,1,0,0)   :   7.+7.j, 
            (1,0,0,1,0,1)   :   7.+5.j, 
            (1,0,0,1,1,0)   :   7.+1.j, 
            (1,0,0,1,1,1)   :   7.+3.j, 
            (1,0,1,0,0,0)   :   5.-7.j, 
            (1,0,1,0,0,1)   :   5.-5.j, 
            (1,0,1,0,1,0)   :   5.-1.j, 
            (1,0,1,0,1,1)   :   5.-3.j, 
            (1,0,1,1,0,0)   :   5.+7.j, 
            (1,0,1,1,0,1)   :   5.+5.j, 
            (1,0,1,1,1,0)   :   5.+1.j, 
            (1,0,1,1,1,1)   :   5.+3.j, 
            (1,1,0,0,0,0)   :   1.-7.j, 
            (1,1,0,0,0,1)   :   1.-5.j, 
            (1,1,0,0,1,0)   :   1.-1.j, 
            (1,1,0,0,1,1)   :   1.-3.j, 
            (1,1,0,1,0,0)   :   1.+7.j, 
            (1,1,0,1,0,1)   :   1.+5.j, 
            (1,1,0,1,1,0)   :   1.+1.j, 
            (1,1,0,1,1,1)   :   1.+3.j, 
            (1,1,1,0,0,0)   :   3.-7.j, 
            (1,1,1,0,0,1)   :   3.-5.j, 
            (1,1,1,0,1,0)   :   3.-1.j, 
            (1,1,1,0,1,1)   :   3.-3.j, 
            (1,1,1,1,0,0)   :   3.+7.j, 
            (1,1,1,1,0,1)   :   3.+5.j, 
            (1,1,1,1,1,0)   :   3.+1.j, 
            (1,1,1,1,1,1)   :   3.+3.j
        }
    else:
        mapping_table = None
        
    initial_value = np.array(list(mapping_table.values()),
                            dtype= complex)
    constellation = Constellation("custom", num_bits_per_symbol = mu, initial_value=initial_value,
                normalize=normalize, center=False, trainable=True)
    if show_constellation:
        constellation.show(figsize=(10, 10))
    return constellation

    

class IEEE80211Scrambler(Layer):
    
    def __init__(self, seed = 127, binary = True, dtype  = tf.float32, batch_size = 64, **kwargs ):
        
        self._binary  = binary
        self._batch_size = batch_size
        if seed is not None:
            self.seed  = seed
        else:
            self.seed   = 127
        
        self.sequence = None
        
        super().__init__(dtype=dtype, **kwargs)

    def _de2bi(self, n, N):
        bseed= bin(n).replace("0b", "")
        fix = N-len(bseed)
        pad = np.zeros(fix)
        pad = pad.tolist()
        y = []
        for i in range(len(pad)):
            y = [int(pad[i])] + y
        for i in range(len(bseed)):
            y = [int(bseed[i])] + y
        return y
            
    def _generate_scrambling(self, len_input_shape):
        
        bit_count =len_input_shape # len(bits)
        
        N=7
        bseed = self._de2bi(self.seed, N)
        bseed = np.array(bseed, dtype=int)
        x = np.zeros((N,), dtype = int)
        x = bseed
        var_arr = np.zeros(bit_count, dtype = int)
        
        
        for n in range(bit_count):
            var = int(x[3])^int(x[6])
            x[1:] = x[0:6]
            x[0] = var
            var_arr[n] = var
        
        self.sequence = tf.convert_to_tensor(var_arr, dtype = tf.int32)          
            
    def build(self, input_shape):
        """Build the model and initialize variables."""
        # self.input_shape = input_shape
        pass
    
    def call(self, inputs):
        is_binary = self._binary # can be overwritten if explicitly provided

        if isinstance(inputs, (tuple, list)):
            if len(inputs)==1: # if user wants to call with call([x])
                seed = None
                x = inputs
            elif len(inputs)==2:
                x, seed = inputs
            elif len(inputs)==3:
            # allow that flag binary is explicitly provided (for descrambler)
                x, seed, is_binary = inputs
                # is binary can be either a tensor or bool
                if isinstance(is_binary, tf.Tensor):
                    if not is_binary.dtype.is_bool:
                        raise TypeError("binary must be bool.")
                else: # is boolean
                    assert isinstance(is_binary.dtype, bool), \
                    "binary must be bool."
            else:
                raise TypeError("inputs cannot have more than 3 entries.")
        else:
            seed = self.seed
            x = inputs
        
        self.len_input_shape = len(x[0,0,0])
        self.seed = seed
        self._generate_scrambling( self.len_input_shape)
        
        self.sequence = tf.tile(tf.expand_dims(tf.expand_dims(tf.expand_dims(self.sequence,axis  = 0),0),0),
                                [self._batch_size,1,1,1]) # TODO remove hardcoded batch size

        # scrambled_bits = tf.zeros(x.shape[0], dtype = tf.int32)
        scrambled_bits = tf.bitwise.bitwise_xor(tf.cast(x, self.sequence.dtype),self.sequence)
                                                
        
        return tf.cast(scrambled_bits, x.dtype)


class IEEE80211Interleaver(Layer):
    
    def __init__(self,
                NCBPS,
                axis=-1,
                inverse=False,
                dtype=tf.float32,
                **kwargs):
        
        self.NCBPS = NCBPS
        self._inverse = inverse
        self._axis = axis
        
        super().__init__(dtype=dtype, **kwargs)
    
    def _generate_perm(self):
        assert self.NCBPS in (48,96,192,288),\
        "currently ony supports NCBPS  = 48,96,192, and 288"
        if self.NCBPS == 48:
            w48 = [1, 17, 33, 2, 18, 34, 3, 19, 35, 4, 20, 36, 5, 21, 37, 6, 22, 38, 7, 23, 39, 8, 24, 40, 9, 25, 41, 10, 26, 42, 11, 27, 43, 12, 28, 44, 13, 29, 45, 14, 30, 46, 15, 31, 47, 16, 32, 48]
            k48 = [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48]
            p =  [(w48[j]-1) for j in range(0, 48)]
            pinv =  [(k48[j]-1) for j in range(0, 48)]
        elif self.NCBPS == 96:
            w96 = [1, 17, 33, 49, 65, 81, 2, 18, 34, 50, 66, 82, 3, 19, 35, 51, 67, 83, 4, 20, 36, 52, 68, 84, 5, 21, 37, 53, 69, 85, 6, 22, 38, 54, 70, 86, 7, 23, 39, 55, 71, 87, 8, 24, 40, 56, 72, 88, 9, 25, 41, 57, 73, 89, 10, 26, 42, 58, 74, 90, 11, 27, 43, 59, 75, 91, 12, 28, 44, 60, 76, 92, 13, 29, 45, 61, 77, 93, 14, 30, 46, 62, 78, 94, 15, 31, 47, 63, 79, 95, 16, 32, 48, 64, 80, 96]
            k96 = [1, 7, 13, 19, 25, 31, 37, 43, 49, 55, 61, 67, 73, 79, 85, 91, 2, 8, 14, 20, 26, 32, 38, 44, 50, 56, 62, 68, 74, 80, 86, 92, 3, 9, 15, 21, 27, 33, 39, 45, 51, 57, 63, 69, 75, 81, 87, 93, 4, 10, 16, 22, 28, 34, 40, 46, 52, 58, 64, 70, 76, 82, 88, 94, 5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96] 
            p = [(w96[j]-1) for j in range(0, 96)]
            pinv = [(k96[j]-1) for j in range(0, 96)]
        elif self.NCBPS == 192:
            w192 = [1, 17, 33, 49, 65, 81, 97, 113, 129, 145, 161, 177, 18, 2, 50, 34, 82, 66, 114, 98, 146, 130, 178, 162, 3, 19, 35, 51, 67, 83, 99, 115, 131, 147, 163, 179, 20, 4, 52, 36, 84, 68, 116, 100, 148, 132, 180, 164, 5, 21, 37, 53, 69, 85, 101, 117, 133, 149, 165, 181, 22, 6, 54, 38, 86, 70, 118, 102, 150, 134, 182, 166, 7, 23, 39, 55, 71, 87, 103, 119, 135, 151, 167, 183, 24, 8, 56, 40, 88, 72, 120, 104, 152, 136, 184, 168, 9, 25, 41, 57, 73, 89, 105, 121, 137, 153, 169, 185, 26, 10, 58, 42, 90, 74, 122, 106, 154, 138, 186, 170, 11, 27, 43, 59, 75, 91, 107, 123, 139, 155, 171, 187, 28, 12, 60, 44, 92, 76, 124, 108, 156, 140, 188, 172, 13, 29, 45, 61, 77, 93, 109, 125, 141, 157, 173, 189, 30, 14, 62, 46, 94, 78, 126, 110, 158, 142, 190, 174, 15, 31, 47, 63, 79, 95, 111, 127, 143, 159, 175, 191, 32, 16, 64, 48, 96, 80, 128, 112, 160, 144, 192, 176]
            k192 = [1, 14, 25, 38, 49, 62, 73, 86, 97, 110, 121, 134, 145, 158, 169, 182, 2, 13, 26, 37, 50, 61, 74, 85, 98, 109, 122, 133, 146, 157, 170, 181, 3, 16, 27, 40, 51, 64, 75, 88, 99, 112, 123, 136, 147, 160, 171, 184, 4, 15, 28, 39, 52, 63, 76, 87, 100, 111, 124, 135, 148, 159, 172, 183, 5, 18, 29, 42, 53, 66, 77, 90, 101, 114, 125, 138, 149, 162, 173, 186, 6, 17, 30, 41, 54, 65, 78, 89, 102, 113, 126, 137, 150, 161, 174, 185, 7, 20, 31, 44, 55, 68, 79, 92, 103, 116, 127, 140, 151, 164, 175, 188, 8, 19, 32, 43, 56, 67, 80, 91, 104, 115, 128, 139, 152, 163, 176, 187, 9, 22, 33, 46, 57, 70, 81, 94, 105, 118, 129, 142, 153, 166, 177, 190, 10, 21, 34, 45, 58, 69, 82, 93, 106, 117, 130, 141, 154, 165, 178, 189, 11, 24, 35, 48, 59, 72, 83, 96, 107, 120, 131, 144, 155, 168, 179, 192, 12, 23, 36, 47, 60, 71, 84, 95, 108, 119, 132, 143, 156, 167, 180, 191]
            p = [(w192[j]-1) for j in range(0, 192)]
            pinv = [(k192[j]-1) for j in range(0, 192)]
        elif self.NCBPS == 288:
            w288 = [1, 17, 33, 49, 65, 81, 97, 113, 129, 145, 161, 177, 193, 209, 225, 241, 257, 273, 18, 34, 2, 66, 82, 50, 114, 130, 98, 162, 178, 146, 210, 226, 194, 258, 274, 242, 35, 3, 19, 83, 51, 67, 131, 99, 115, 179, 147, 163, 227, 195, 211, 275, 243, 259, 4, 20, 36, 52, 68, 84, 100, 116, 132, 148, 164, 180, 196, 212, 228, 244, 260, 276, 21, 37, 5, 69, 85, 53, 117, 133, 101, 165, 181, 149, 213, 229, 197, 261, 277, 245, 38, 6, 22, 86, 54, 70, 134, 102, 118, 182, 150, 166, 230, 198, 214, 278, 246, 262, 7, 23, 39, 55, 71, 87, 103, 119, 135, 151, 167, 183, 199, 215, 231, 247, 263, 279, 24, 40, 8, 72, 88, 56, 120, 136, 104, 168, 184, 152, 216, 232, 200, 264, 280, 248, 41, 9, 25, 89, 57, 73, 137, 105, 121, 185, 153, 169, 233, 201, 217, 281, 249, 265, 10, 26, 42, 58, 74, 90, 106, 122, 138, 154, 170, 186, 202, 218, 234, 250, 266, 282, 27, 43, 11, 75, 91, 59, 123, 139, 107, 171, 187, 155, 219, 235, 203, 267, 283, 251, 44, 12, 28, 92, 60, 76, 140, 108, 124, 188, 156, 172, 236, 204, 220, 284, 252, 268, 13, 29, 45, 61, 77, 93, 109, 125, 141, 157, 173, 189, 205, 221, 237, 253, 269, 285, 30, 46, 14, 78, 94, 62, 126, 142, 110, 174, 190, 158, 222, 238, 206, 270, 286, 254, 47, 15, 31, 95, 63, 79, 143, 111, 127, 191, 159, 175, 239, 207, 223, 287, 255, 271, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 272, 288]
            k288 = [1, 21, 38, 55, 75, 92, 109, 129, 146, 163, 183, 200, 217, 237, 254, 271, 2, 19, 39, 56, 73, 93, 110, 127, 147, 164, 181, 201, 218, 235, 255, 272, 3, 20, 37, 57, 74, 91, 111, 128, 145, 165, 182, 199, 219, 236, 253, 273, 4, 24, 41, 58, 78, 95, 112, 132, 149, 166, 186, 203, 220, 240, 257, 274, 5, 22, 42, 59, 76, 96, 113, 130, 150, 167, 184, 204, 221, 238, 258, 275, 6, 23, 40, 60, 77, 94, 114, 131, 148, 168, 185, 202, 222, 239, 256, 276, 7, 27, 44, 61, 81, 98, 115, 135, 152, 169, 189, 206, 223, 243, 260, 277, 8, 25, 45, 62, 79, 99, 116, 133, 153, 170, 187, 207, 224, 241, 261, 278, 9, 26, 43, 63, 80, 97, 117, 134, 151, 171, 188, 205, 225, 242, 259, 279, 10, 30, 47, 64, 84, 101, 118, 138, 155, 172, 192, 209, 226, 246, 263, 280, 11, 28, 48, 65, 82, 102, 119, 136, 156, 173, 190, 210, 227, 244, 264, 281, 12, 29, 46, 66, 83, 100, 120, 137, 154, 174, 191, 208, 228, 245, 262, 282, 13, 33, 50, 67, 87, 104, 121, 141, 158, 175, 195, 212, 229, 249, 266, 283, 14, 31, 51, 68, 85, 105, 122, 139, 159, 176, 193, 213, 230, 247, 267, 284, 15, 32, 49, 69, 86, 103, 123, 140, 157, 177, 194, 211, 231, 248, 265, 285, 16, 36, 53, 70, 90, 107, 124, 144, 161, 178, 198, 215, 232, 252, 269, 286, 17, 34, 54, 71, 88, 108, 125, 142, 162, 179, 196, 216, 233, 250, 270, 287, 18, 35, 52, 72, 89, 106, 126, 143, 160, 180, 197, 214, 234, 251, 268, 288]
            p = [(w288[j]-1) for j in range(0, 288)]
            pinv = [(k288[j]-1) for j in range(0, 288)]

        return np.array(p, dtype = np.int32), np.array(pinv, dtype = np.int32)
    
    def build(self, input_shape):
        assert self._axis < len(input_shape), "Axis does match input shape"
        # init rand sequences during build
        assert input_shape[self._axis] is not None, "Unknown shape at req. dim"
        p, pinv = self._generate_perm()

        multiples = input_shape[-1]//self.NCBPS
        p = np.tile(p, (multiples,))
        pinv = np.tile(pinv, (multiples,))

        for ind in range(0,multiples):
          p[ind*self.NCBPS: (ind+1)*self.NCBPS] += ind*self.NCBPS
          pinv[ind*self.NCBPS: (ind+1)*self.NCBPS] += ind*self.NCBPS
        
        
        self._perm_seq = tf.convert_to_tensor(p, dtype = tf.int32)
        self._perm_seq_inv = tf.convert_to_tensor(pinv, dtype = tf.int32)
        # print("self._perm_seq_inv.shape")
        # print(self._perm_seq_inv.shape)
        # self._perm_seq = tf.tile(tf.convert_to_tensor(p), [input_shape[-1]//self.NCBPS])
        # self._perm_seq_inv = tf.tile(tf.convert_to_tensor(pinv), [input_shape[-1]//self.NCBPS])
        
        
    def call(self, inputs):    

        input_shape = inputs.shape

        # re-init if shape has changed, update perm_seq
        # if inputs.shape[self._axis] != self._perm_seq.shape[0]:
        #     self.build(inputs.shape)

        # print("inputs shape interleaver"); print(inputs.shape)
        if self._inverse:
            x = tf.gather(inputs, self._perm_seq_inv, axis = 3)
            # x = inputs[self._perm_seq_inv]
        else:
            x = tf.gather(inputs, self._perm_seq, axis = 3)
            # x = inputs[self._perm_seq]

        x = tf.ensure_shape(x, input_shape)
        return x

"""# Config Model OFDM params"""

############################################
## Channel configuration
carrier_frequency = 5.9e9 # Hz
delay_spread = 100e-9 # s
cdl_model = "C" # CDL model to use
speed = 10.0 # Speed for evaluation and training [m/s]


############################################
## OFDM waveform configuration
subcarrier_spacing = 312.5e3/2 # Hz
fft_size = 64 # Number of subcarriers forming the resource grid, including the null-subcarrier and the guard bands
num_ofdm_symbols = 20 # Number of OFDM symbols forming the resource grid
dc_null = True # Null the DC subcarrier
num_guard_carriers = [5, 6] # Number of guard carriers on each side
pilot_pattern = "kronecker" # Pilot pattern
pilot_ofdm_symbol_indices = [2, 11] # Index of OFDM symbols carrying pilots
cyclic_prefix_length = 0 # Simulation in frequency domain. This is useless

############################################
## Modulation and coding configuration
num_bits_per_symbol = 2 # 4: 16-QAM
print(f"num_bits_per_symbol : {num_bits_per_symbol}")
if num_bits_per_symbol == 1: NCBPS  = 48
elif num_bits_per_symbol == 2: NCBPS  = 96
elif num_bits_per_symbol == 4: NCBPS  = 192
elif num_bits_per_symbol == 6: NCBPS  = 288
else: NCBPS  = None
constellation = generate_ieee8211_constellation(mu  = num_bits_per_symbol)

coderate = 0.5 # Coderate for BCC code

############################################
## Neural receiver configuration
num_conv_channels = 128 #128 # Number of convolutional channels for the convolutional layers forming the neural receiver

############################################
## Training configuration
num_training_iterations = 1000#30000 # Number of training iterations
training_batch_size = 64 # Training batch size
model_weights_path = "llr_net_perfect_csi_weights"+"_R_"+'{:1.2f}'.format(coderate)+"_mu_" + '{:1}'.format(num_bits_per_symbol) # Location to save the neural receiver weights once training is done

############################################
## Evaluation configuration
results_filename = "neural_receiver_results" # Location to save the results


############################################
## Antenna configuration
stream_manager = StreamManagement(np.array([[1]]), # Receiver-transmitter association matrix
                                  1)               # One stream per transmitter
ut_antenna = Antenna(polarization="single",
                     polarization_type="V",
                     antenna_pattern="38.901",
                     carrier_frequency=carrier_frequency)

bs_array = AntennaArray(num_rows=1,
                        num_cols=1,
                        polarization="dual",
                        polarization_type="VH",
                        antenna_pattern="38.901",
                        carrier_frequency=carrier_frequency)

"""# 2. Neural Receiver"""

#!/usr/bin/env python
# coding: utf-8




short_training_symbol=math.sqrt(13/6) * np.array([0, 0, 1+1j, 0, 0, 0, -1-1j, 0, 0, 0,
    1+1j, 0, 0, 0, -1-1j, 0, 0, 0, -1-1j, 0, 0, 0, 1+1j, 0, 0, 0, 0, 0,
    0, 0, -1-1j, 0, 0, 0, -1-1j, 0, 0, 0, 1+1j, 0, 0, 0, 1+1j, 0, 0, 0,
    1+1j, 0, 0, 0, 1+1j, 0,0], dtype=complex)
# print(len(short_training_symbol))

long_training_symbol=np.array([1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1,
    -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 0, 1, -1, -1, 1, 1, -1, 1, -1,
    1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1])
# print(len(long_training_symbol))

pilots=np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, -1, 0, 0, 0, 0, 0])
# print(len(pilots))

pilotpolarity=np.array([1,1,1,1, -1,-1,-1,1, -1,-1,-1,-1, 1,1,-1,1, -1,-1,1,1,
    -1,1,1,-1, 1,1,1,1, 1,1,-1,1,1,1,-1,1, 1,-1,-1,1, 1,1,-1,1,
    -1,-1,-1,1, -1,1,-1,-1, 1,-1,-1,1, 1,1,1,1, -1,-1,1,1,-1,-1,1,-1, 
    1,-1,1,1, -1,-1,-1,1, 1,-1,-1,-1, -1,1,-1,-1, 1,-1,1,1, 1,1,-1,1, 
    -1,1,-1,1,-1,-1,-1,-1, -1,1,-1,1, 1,-1,1,-1, 1,1,1,-1, -1,1,-1,-1,
    -1,1,1,1, -1,-1,-1,-1, -1,-1,-1])
# print(len(pilotpolarity))

def generate_pilot_loc_matrix(N):
    '''
    @param N: symbols additionally added to two long training symbols, and one ...
                data symbol that contains four pilots in subcarriers.
    Generate Pilot location matrix for 802.11p 
    '1' indicates a pilot, '0' indicates data or Null
    Including block and comb type pilots as defined by the standard
    Note: Null subcarriers are also included.
    Returns a matrix of 64 x (N+3)
    '''
    p1 = 7 # subcarrier Pilot location
    p2 = 21 # subcarrier Pilot location
    p3 = 43
    p4 = 57
    # N = 12
    long_pilot_loc = [0 if (ind in range(27,38) or ind == 0) else val for ind, val in enumerate(itertools.repeat(1,64))]
    # len(long_pilot_loc)
    short_pilot_loc = [1 if (ind == p1  or ind == p2 or ind == p3 or ind == p4 ) else val for ind, val in enumerate(itertools.repeat(0,64))]
    # len(short_pilot_loc)
    pilot_matrix_base = list(map(list, zip(long_pilot_loc, long_pilot_loc, short_pilot_loc)))
    pilot_matrix_base = np.array(pilot_matrix_base)
    short_pilots_rep = np.hstack([pilot_matrix_base[:,2].reshape((-1, 1))]*N)
    pilot_matrix_loc = np.concatenate((pilot_matrix_base,short_pilots_rep), 1)
    return pilot_matrix_loc

def generate_pilot_pattern_80211p(num_tx = 1,num_streams_per_tx = 1, num_ofdm_symbols = 3, num_effective_subcarriers = 52, combPilotOnly = False):
    '''
    #num_tx, num_streams_per_tx, num_ofdm_symbols, num_effective_subcar
    '''
    # Generate MASK
    num_ofdm_symbols -=3
    if combPilotOnly:
        num_pilot_symbols = 4*(num_ofdm_symbols -1)
        mask = np.zeros((num_tx,
                 num_streams_per_tx,
                 num_ofdm_symbols -1,
                 num_effective_subcarriers))
        pilots = np.zeros((num_tx,
                 num_streams_per_tx,
                 num_pilot_symbols),
                 dtype=complex)
        pilot_matrix_loc = generate_pilot_loc_matrix(num_ofdm_symbols-3)
    else:
        num_pilot_symbols =52+52 + 4*(num_ofdm_symbols -1)
        mask = np.zeros((num_tx,
                 num_streams_per_tx,
                 num_ofdm_symbols+3,
                 num_effective_subcarriers))
        pilots = np.zeros((num_tx,
                 num_streams_per_tx,
                 num_pilot_symbols +2*4),
                 dtype=complex)
        pilot_matrix_loc = generate_pilot_loc_matrix(num_ofdm_symbols)


    pilot_matrix_loc = np.roll(pilot_matrix_loc[1:],26, axis=0)[0:52].T

    # Generate CombPilots
    ij =2 
    NST = num_effective_subcarriers  #52 + 1 # imprecise Including DC for now
    pilot1 = np.array([1,1,1,-1])
    pilotIndex1 = np.block([4, 18, 33, 47]).tolist()
    # combGrid = np.zeros((num_ofdm_symbols-4,NST),dtype=complex)
    combPilots = np.zeros(num_pilot_symbols -52-52 +2*4,dtype=complex)
    for ii in range(0,num_ofdm_symbols -1 +2):
        inputiFFT1 = np.zeros(NST,dtype = complex)
        inputiFFT1[pilotIndex1] = np.zeros(len(pilotIndex1),dtype = complex)
        ij  = ij%127
        pm = pilotpolarity[ij]
        ij = ij+1
        pilot2 = pm*pilot1
        combPilots[4*ii:4*(ii+1)] = pilot2
        # #Pilots Insertion
        # inputiFFT1[5]=pilot2[0]
        # inputiFFT1[19]=pilot2[1]
        # inputiFFT1[33]=pilot2[2]
        # inputiFFT1[47]=pilot2[3]
        # combGrid[ii, :] = inputiFFT1

    if combPilotOnly:
         pilot_matrix_loc = pilot_matrix_loc[2:][:]
    else:
        pilot_matrix_loc = pilot_matrix_loc[:][:]
    # Block Pilots
    BlockPilot = np.delete(long_training_symbol,26)
         
    for indtx in range(0,num_tx):
        for indstreamtx in range(0,num_streams_per_tx):
            mask[indtx, indstreamtx, :, :] = pilot_matrix_loc
            print(mask.shape)
            print(int(np.sum(mask[0,0])))
            if combPilotOnly:
                pilots[indtx, indstreamtx, :] = combPilots
            else:
                pilots[indtx, indstreamtx, :] = np.block([ BlockPilot, BlockPilot, combPilots])
                print(pilots.shape)
    with tf.device('/cpu:0'):
        pilot_pattern_80211p = PilotPattern(mask = mask, pilots = pilots, trainable=False, normalize=False, dtype=tf.complex64)

    return pilot_pattern_80211p


Nsym = num_ofdm_symbols
pilot_pattern_80211p = generate_pilot_pattern_80211p(num_tx = 1,num_streams_per_tx = 1, num_ofdm_symbols = Nsym, num_effective_subcarriers = 52, combPilotOnly = False)
num_guard_carriers = [6, 5]


resource_grid = ResourceGrid(num_ofdm_symbols = num_ofdm_symbols,
                             fft_size = fft_size,
                             subcarrier_spacing = subcarrier_spacing,
                             num_tx = 1,
                             num_streams_per_tx = 1,
                             cyclic_prefix_length = cyclic_prefix_length,
                             dc_null = dc_null,
                             pilot_pattern = pilot_pattern_80211p,
                             num_guard_carriers = num_guard_carriers) 

print(f"num_data_symbols: {resource_grid.num_data_symbols}")


resource_grid.show()



# Codeword length. It is calculated from the total number of databits carried by the resource grid, and the number of bits transmitted per resource element
n = int(resource_grid.num_data_symbols*num_bits_per_symbol)
# Number of information bits per codeword
k = int(n*coderate) 



class ResidualBlock(Layer):
    r"""
    This Keras layer implements a convolutional residual block made of two convolutional layers with ReLU activation, layer normalization, and a skip connection.
    The number of convolutional channels of the input must match the number of kernel of the convolutional layers ``num_conv_channel`` for the skip connection to work.
    
    Input
    ------
    : [batch size, num time samples, num subcarriers, num_conv_channel], tf.float
        Input of the layer
    
    Output
    -------
    : [batch size, num time samples, num subcarriers, num_conv_channel], tf.float
        Output of the layer
    """
                        
    def build(self, input_shape):
        
        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'
        self._layer_norm_1 = LayerNormalization(axis=(-1, -2, -3))
        self._conv_1 = Conv2D(filters=num_conv_channels,
                              kernel_size=[3,3],
                              padding='same',
                              activation=None)
        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'
        self._layer_norm_2 = LayerNormalization(axis=(-1, -2, -3))
        self._conv_2 = Conv2D(filters=num_conv_channels,
                              kernel_size=[3,3],
                              padding='same',
                              activation=None)
    
    def call(self, inputs):
        z = self._layer_norm_1(inputs)
        z = relu(z)
        z = self._conv_1(z)
        z = self._layer_norm_2(z)
        z = relu(z)
        z = self._conv_2(z) # [batch size, num time samples, num subcarriers, num_channels]
        # Skip connection
        z = z + inputs
        
        return z


########################################################################################################################
# -------------------LLRNET--------------------#

def qfunc(arg):
    return 0.5 - 0.5 * sp.erf(arg / 1.414)

QAM_64 = [[4, 12, 28, 20, 52, 60, 44, 36],
          [5, 13, 29, 21, 53, 61, 45, 37],
          [7, 15, 31, 23, 55, 63, 47, 39],
          [6, 14, 30, 22, 54, 62, 46, 38],
          [2, 10, 26, 18, 50, 58, 42, 34],
          [3, 11, 27, 19, 51, 59, 43, 35],
          [1, 9, 25, 17, 49, 57, 41, 33],
          [0, 8, 24, 16, 48, 56, 40, 32]]

QAM_16 = [[2, 6, 14, 10],
          [3, 7, 15, 11],
          [1, 5, 13, 9],
          [0, 4, 12, 8]]

QAM_4 = [[1, 3],
         [0, 2]]

BPSK_2 = [[0, 1]]

PSK_8 = [[0, 1, 5, 4, 6, 7, 3, 2]]

QAM_64_b = [['000100', '001100', '011100', '010100', '110100', '111100', '101100', '100100'],
            ['000101', '001101', '011101', '010101', '110101', '111101', '101101', '100101'],
            ['000111', '001111', '011111', '010111', '110111', '111111', '101111', '100111'],
            ['000110', '001110', '011110', '010110', '110110', '111110', '101110', '100110'],
            ['000010', '001010', '011010', '010010', '110010', '111010', '101010', '100010'],
            ['000011', '001011', '011011', '010011', '110011', '111011', '101011', '100011'],
            ['000001', '001001', '011001', '010001', '110001', '111001', '101001', '100001'],
            ['000000', '001000', '011000', '010000', '110000', '111000', '101000', '100000']]

QAM_16_b = [['0010', '0110', '1110', '1010'],
            ['0011', '0111', '1111', '1011'],
            ['0001', '0101', '1101', '1001'],
            ['0000', '0100', '1100', '1000']]

QAM_4_b = [['01', '11'],
           ['00', '10']]

BPSK_2_b = [['0', '1']]

PSK_8_b = [['000', '001', '101', '100', '110', '111', '011', '010']]


class Modulation_Map():
    def __init__(self, modulation):
        self.modulation = modulation
        if modulation == '4QAM':
            self.number_matrix, self.binary_matrix = QAM_4, QAM_4_b
        elif modulation == '16QAM':
            self.number_matrix, self.binary_matrix = QAM_16, QAM_16_b
        elif modulation == '64QAM':
            self.number_matrix, self.binary_matrix = QAM_64, QAM_64_b
        elif modulation == 'BPSK':
            self.number_matrix, self.binary_matrix = BPSK_2, BPSK_2_b
        elif modulation == '8PSK':
            self.number_matrix, self.binary_matrix = PSK_8, PSK_8_b
        self.create_num_to_bin_dictionary()
        self.create_bin_to_coordinate_dictionary()

    def create_num_to_bin_dictionary(self):
        number_matrix = self.number_matrix
        binary_matrix = self.binary_matrix
        n, d = len(number_matrix), len(number_matrix[0])
        num_to_bin = {}
        for i in range(n):
            for j in range(d):
                key = number_matrix[i][j]
                value = binary_matrix[i][j]
                num_to_bin[key] = value
        self.num_to_bin = num_to_bin

    def create_bin_to_coordinate_dictionary(self):
        number_matrix = self.number_matrix
        binary_matrix = self.binary_matrix
        n, d = len(number_matrix), len(number_matrix[0])
        bin_to_coordinate = {}
        for i in range(n):
            for j in range(d):
                key = binary_matrix[i][j]
                if self.modulation == '4QAM':
                    value = -1 + 2 * j, -1 + 2 * i
                elif self.modulation == '16QAM':
                    value = -3 + 2 * i, -3 + 2 * j
                elif self.modulation == '64QAM':
                    value = -7 + 2 * i, -7 + 2 * j
                elif self.modulation == 'BPSK':
                    value = -1 + 2 * j, i
                else:  # 8PSK
                    value = math.sin(j * math.pi / 4), math.cos(j * math.pi / 4)
                bin_to_coordinate[key] = value
                self.bin_to_coordinate = bin_to_coordinate




class Transmitter_Receiver():
    def __init__(self, num_bits_send, modulation, llr_calc='exact', scale_modulation=True):
        self.num_bits_send = num_bits_send
        self.modulation = modulation
        self.modulation_map = Modulation_Map(modulation)
        self.Scrambling = True
        self.Interleaving = True
        self.llr_calc = llr_calc
        self.scale_modulation = scale_modulation

        if modulation == '4QAM':
            self.M = 4;
            self.k = 2;
            self.NCBPS = 96
            self.binary_matrix = QAM_4_b
        elif modulation == '16QAM':
            self.M = 16
            self.k = 4;
            self.NCBPS = 192;
            self.binary_matrix = QAM_16_b
        elif modulation == '64QAM':
            self.M = 64;
            self.k = 6;
            self.NCBPS = 288;
            self.binary_matrix = QAM_64_b
        elif modulation == 'BPSK':
            self.M = 2;
            self.k = 1;
            self.NCBPS = 48;
            self.binary_matrix = BPSK_2_b
        else:  # 8PSK
            self.M = 8;
            self.k = 3;
            self.binary_matrix = PSK_8_b

        self.scaling_factor = self.get_scaling()

    def send_n_receive(self, snr, verbose=False):
        self.snr = snr
        N_0 = self.snr_to_N0()
        self.N_0 = N_0
        if verbose:
            print('Sending %d bits with snr = %fdB' % (self.num_bits_send, snr))
        self.generate_bits()
        self.serial_to_parallel()
        y = self.bit_stream_to_constellation()
        r = self.add_noise(y, N_0 / 2)
        self.r = r
        if verbose:
            plt.scatter(r[0, :], r[1, :])
            print('Finished Sending %d bits with snr = %fdB\n' % (self.num_bits_send, snr))

    def decode(self, verbose=False):
        if verbose:
            print('Decoding %d bits...' % (self.num_bits_send))
        r = self.r
        # print(r.shape)
        # print(r)
        n, d = np.shape(r)
        llr = np.zeros((self.k, d))
        decoded = np.zeros((self.k, d))
        for i in range(d):
            if self.llr_calc == 'approx':
                single_message_llr = self.r_to_llr_approx(r[:, i])
            if self.llr_calc == 'exact':
                single_message_llr = self.r_to_llr(r[:, i])
            # print(single_message_llr)
            llr[:, i] = np.flip(single_message_llr)
            decoded[:, i] = np.flip((1 - (single_message_llr / np.abs(single_message_llr))) / 2)

        bit_error = np.sum(np.abs(decoded - self.bit_stream))
        bit_error_rate = bit_error / self.num_bits_send
        self.bit_error = bit_error
        self.bit_error_rate = bit_error_rate
        self.llr = llr
        self.decoded = decoded
        if verbose:
            print('SNR level...%d' % (self.snr))
            print('Finished decoding %d bits...' % (self.num_bits_send))
            print('Number of Bit Errors: {}'.format(bit_error))
            print('Bit Error Rate:{}\n'.format(bit_error_rate))

    def get_scaling(self):
        scaling_dict = {1: 1, 2: np.sqrt(2), 4: np.sqrt(10), 6: np.sqrt(42)}
        k = self.k
        modulation = self.modulation
        if not self.scale_modulation:
            scaling_factor = 1
        else:
            scaling_factor = scaling_dict[k]
        return scaling_factor

    def r_to_llr(self, r):
        '''
    Returns: k array of llrs
    '''
        # print(r.shape)
        # print(r)
        k = self.k
        modulation_map = self.modulation_map
        llr = np.zeros(k)
        for i in np.arange(k):
            zero_sum, one_sum = 0., 0.
            num, den = 0., 0.
            num_values = [];
            den_values = []
            SCIPYLOGSUMEXP = False
            if SCIPYLOGSUMEXP:
                for key in modulation_map.bin_to_coordinate.keys():

                    r_ = modulation_map.bin_to_coordinate[key]
                    r_ = np.array([r_[0], r_[1]])
                    scale = 1 / self.scaling_factor
                    # custom_norm = np.abs(r[0] +1j*r[1])**2 +  np.abs(r_[0] +1j*r_[1])**2 -2*(r[0]*r_[0] + r[1]*r_[1])
                    exponent = -1.0 * (np.linalg.norm(scale * (r_ - r)) ** 2) / (2 * self.snr_to_N0_bpsk())
                    # exponent = -1 * custom_norm/(self.N_0)
                    if key[k - i - 1] == '0':
                        num_values.append(exponent)
                    else:
                        den_values.append(exponent)

                llr[i] = logsumexp(np.array(num_values)) - logsumexp(np.array(den_values))
                return llr
            else:

                for key in modulation_map.bin_to_coordinate.keys():
                    r_ = modulation_map.bin_to_coordinate[key]
                    r_ = np.array([r_[0], r_[1]])
                    scale = 1 / self.scaling_factor

                    exponent = -1.0 * (np.linalg.norm(scale * (r_ - r)) ** 2) / (2 * self.snr_to_N0_bpsk())
                    total = np.exp(exponent)
                    if key[k - i - 1] == '0':
                        zero_sum += total
                    else:
                        one_sum += total

                num = 0 if zero_sum == 0 else zero_sum
                den = 0 if one_sum == 0 else one_sum
                try:
                    num = np.log(zero_sum)
                    den = np.log(one_sum)
                except:
                    print(f"k = {k}")

            llr[i] = num - den
            llr[i] = 10 ** -5 if llr[i] == 0 else llr[i]
        return llr

    def r_to_llr_approx(self, r):
        '''
    Returns: k array of llrs
    '''
        k = self.k
        modulation_map = self.modulation_map
        llr = np.zeros(k)
        scale = 1 / self.scaling_factor
        for i in range(k):
            zero_sum, one_sum = [], []
            for key in modulation_map.bin_to_coordinate.keys():
                r_ = modulation_map.bin_to_coordinate[key]

                r_ = np.array([r_[0], r_[1]])
                # print(f"r_:  {r_.shape}")

                exponent = np.linalg.norm(scale * (r - r_)) ** 2
                total = exponent
                if key[k - i - 1] == '0':
                    zero_sum.append(total)
                else:
                    one_sum.append(total)
            llr[i] = (min(one_sum) - min(zero_sum)) * (1 / (2 * self.snr_to_N0_bpsk()))
            llr[i] = 10 ** -5 if llr[i] == 0 else llr[i]
        return llr

    def snr_to_N0(self):
        '''
    Returns: float N0
    '''
        k, M, snr = self.k, self.M, 10 ** (self.snr / 10)
        sum = 0
        for value in self.modulation_map.bin_to_coordinate.values():
            sum += value[0] ** 2 + value[1] ** 2
        e_avg = sum / M

        return e_avg / (k * snr)

    def snr_to_N0_bpsk(self):
        '''
    Returns: float N0 for BPSK
    '''
        k, M, snr = 1, 2, 10 ** (self.snr / 10)
        e_avg = 1
        return e_avg / (k * snr)

    def bi2de(self, binary):
        x = 0
        for n in range(0, len(binary)):
            x = (binary[n] * (2 ** n)) + x
        return x

    def de2bi(self, n, N):
        bseed = bin(n).replace("0b", "")
        fix = N - len(bseed)
        pad = np.zeros(fix)
        pad = pad.tolist()
        y = []
        for i in range(len(pad)):
            y = [int(pad[i])] + y
        for i in range(len(bseed)):
            y = [int(bseed[i])] + y
        return y

    def scrambler(self, seed=[1, 0, 1, 1, 1, 0, 1]):
        # initialize scrambler
        # scramble
        bits = self.bit_stream.ravel()
        bit_count = len(bits)
        scrambled_bits = np.zeros(bit_count)
        N = 7
        seed = self.bi2de(seed)
        bseed = self.de2bi(seed, N)
        x1 = bseed[0]
        x2 = bseed[1]
        x3 = bseed[2]
        x4 = bseed[3]
        x5 = bseed[4]
        x6 = bseed[5]
        x7 = bseed[6]

        for n in range(bit_count):
            x1t = x1
            x2t = x2
            x3t = x3
            x4t = x4
            x5t = x5
            x6t = x6
            x7t = x7
            var = int(x4t) ^ int(x7t)
            scrambled_bits[n] = int(var) ^ int(bits[n])
            x1 = var
            x2 = x1t
            x3 = x2t
            x4 = x3t
            x5 = x4t
            x6 = x5t
            x7 = x6t

        self.bit_stream = scrambled_bits.reshape((-1, 1))

    def generate_bits(self):
        '''
    Sets bit_stream to: num_bits_send, 1 array of bits
    '''
        # make num_bits_send a multiple of Nbpsc
        # e.g Nbpsc =  4 for 16-QAM
        # Nbits = self.num_bits_send + self.k - (self.num_bits_send % self.k)
        Nbits = self.num_bits_send
        # self.num_bits_send = Nbits
        bit_stream = np.around(np.random.rand(Nbits, 1))
        self.bit_stream = bit_stream

        # Scramble bits so there are not many repeated ones or zeros.

        if self.Scrambling:
            self.scrambler()

    def serial_to_parallel(self):
        '''
    Sets bit_stream to: k, n/k array of bits
    '''
        k = self.k
        c = self.bit_stream
        n, d = np.shape(c)

        if n % k == 0:
            copy = np.transpose(c.reshape((int(n / k), k)))
            self.bit_stream = copy
        else:
            copy = np.append(c, np.zeros((k - n % k, 1)))
            n_, = np.shape(copy)
            self.bit_stream = np.transpose(copy.reshape((int(n_ / k), k)))

    def bit_stream_to_constellation(self):
        '''
    Returns: 2,d array of 2-D coordinate points
    '''
        modulation = self.modulation
        modulation_map = self.modulation_map
        bit_stream = self.bit_stream
        n, d = np.shape(bit_stream)
        constellation = np.zeros((2, d))

        for j in range(d):
            stream = bit_stream[:, j]
            key = ''
            for bit in stream:
                key += str(int(bit))
            x1, y1 = modulation_map.bin_to_coordinate[key]
            constellation[0, j], constellation[1, j] = x1, y1
        return constellation

    def add_noise(self, y, var):
        n, d = np.shape(y)
        return y + math.sqrt(var) * np.random.randn(n, d)

# In[58]:


def theoretical_bit_error_rate(modulation, SNR_dB):
    snr = 10 ** (SNR_dB / 10)
    if modulation == 'BPSK':
        bit_error = qfunc((2 * snr) ** 0.5)
    elif modulation == '8PSK':
        M = 8
        bit_error = 2 / ((math.log2(M))) * qfunc(math.sin(math.pi / M) * (2 * snr * math.log2(M)) ** 0.5)
    elif modulation == '4QAM' or modulation == '16QAM' or modulation == '64QAM':
        M = 4 if modulation == '4QAM' else 16 if modulation == '16QAM' else 64
        bit_error = 4 / ((math.log2(M))) * qfunc((3 * snr * math.log2(M) / (M - 1)) ** 0.5)
    else:
        print('Must choose valid modulation')
        return 0
    return bit_error


NAME = ''
tensorboard = None


def init_tensorboard(modulation, train_SNR, llr_calc):
    global NAME
    global tensorboard
    NAME = "LLRnet-{}-snr{}-{}-{}".format(modulation, train_SNR, llr_calc, datetime.now().strftime("%Y%m%d_%H%M%S%U"))
    tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))

class LLR_net():
    def __init__(self, modulation, training_size, test_size, train_snr=10,
                 test_snr=10, epoch_size=50, activation='tanh',
                 optimizer='adam', loss='mse', neuron_size=16,
                 layer_num=0, llr_calc='exact', input_dim=2, single_SNR=True):
        """


    :param modulation: 'BPSK' | '4QAM' | '16QAM' | '64QAM'
    :param training_size: 50000
    :param test_size: 10000
    :param train_snr: 10
    :param test_snr: 5
    :param epoch_size: 30
    :param activation: 'relu' | 'tanh'
    :param optimizer: 'adam' | 'sgd'
    :param loss: 'binary_crossentropy' | 'mse'
    :param neuron_size: K  =  8 |16 | 32 based on Modulation order
    :param layer_num: default = 0 (1st layer only), additional hidden to the 1st hidden
    :param llr_calc: 'exact' or 'approx' for training
    :param input_dim: 2 | 3
    """
        self.llr_calc = llr_calc
        self.activation = activation
        self.modulation = modulation
        self.neuron_size = 16 if self.modulation == '64QAM' else 8
        # self.neuron_size = neuron_size
        self.loss, self.optimizer = loss, optimizer
        self.train_snr, self.test_snr = train_snr, test_snr
        self.training_size, self.test_size = training_size, test_size
        self.model_history = {}
        self.input_dim = input_dim
        self.single_SNR = single_SNR
        init_tensorboard(self.modulation, self.train_snr, self.llr_calc)
        '''
    Model Definition: Sequential
    Model Paramters: The parameters can be specified in the declaration. Optimization of Parameters
                      are given in next sections
    Note:
    Last Layer is defined in the training system to account for different 'k' values
    '''
        model = Sequential()
        # Input Layer and First hidden layer
        model.add(Dense(self.neuron_size, input_dim=self.input_dim, activation=self.activation))
        # Extra Hidden Layers
        for i in range(layer_num):
            model.add(Dropout(0.2))
            model.add(Dense(self.neuron_size, activation=self.activation))
        self.model = model
        self.epoch_size = int(epoch_size)


    def train(self, wide_snrRange=[-10, 15], verbose=1):
        '''
    Generates Training Data by
    1. Defining a Transmitter/Receiver for the given number of training bits, modulation and snr
    2. Maps the bits to constellation with AWGN -> This is the Training Data, X
    3. Decodes using tradition decoding -> This is the Training Data, y
    4. Trains the Neural Network using the Training Data
    '''
        if self.single_SNR:

            self.training_system = Transmitter_Receiver(self.training_size, self.modulation, self.llr_calc)
            self.training_system.send_n_receive(self.train_snr)
            self.training_system.decode(verbose=1)
            if self.input_dim == 3:
                N_0_all = np.repeat([self.training_system.N_0],
                                    self.training_system.r.shape[1], axis=0).reshape(1, -1)
                self.X = np.transpose(np.concatenate((self.training_system.r, N_0_all), axis=0))
            if self.input_dim == 2:
                self.X = np.transpose(self.training_system.r)
            self.y = np.transpose(self.training_system.llr)
        else:
            self.wide_snrRange = wide_snrRange
            SNR_Range = self.wide_snrRange
            spacing = 1
            snr_list = np.linspace(SNR_Range[0], SNR_Range[1],
                                   int(np.abs(SNR_Range[0] - SNR_Range[1]) * 1 / spacing) + 1)
            train_size = self.training_size // len(snr_list)
            # train_size = 1000
            self.training_system = Transmitter_Receiver(train_size, self.modulation, self.llr_calc)

            k = self.training_system.k

            # print(f"X: {X.shape}"); print(f"y: {X.shape}")

            for snr_index, snr in enumerate(snr_list):
                self.training_system.send_n_receive(self.train_snr)
                self.training_system.decode(verbose=1)
                if snr_index == 0:
                    train_size = self.training_system.r.shape[1]
                    if self.input_dim == 3:
                        self.X = np.zeros((train_size * len(snr_list), 3), dtype=float)
                    if self.input_dim == 2:
                        self.X = np.zeros((train_size * len(snr_list), 2), dtype=float)
                    self.y = np.zeros((train_size * len(snr_list), self.training_system.k), dtype=float)
                if self.input_dim == 3:
                    N_0_all = np.repeat([self.training_system.N_0],
                                        self.training_system.r.shape[1], axis=0).reshape(1, -1)
                    X_i = np.transpose(np.concatenate((self.training_system.r, N_0_all), axis=0))
                if self.input_dim == 2:
                    X_i = np.transpose((self.training_system.r))
                y_i = np.transpose(self.training_system.llr)
                # print(f"r: {self.training_system.r.shape}"); print(f"X_i: {X_i.shape}"); print(f"y_i: {y_i.shape}")
                self.X[snr_index * train_size:(snr_index + 1) * train_size, :] = X_i
                self.y[snr_index * train_size:(snr_index + 1) * train_size, :] = y_i

        # Output Layer
        self.model.add(Dense(self.training_system.k, activation='linear'))
        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['mse'])
        print(f"X: {self.X.shape}");
        print(f"y: {self.y.shape}")
        self.model_history = self.model.fit(self.X, self.y, epochs=self.epoch_size, validation_split=0.3, batch_size=50,
                                            callbacks=[tensorboard], verbose=verbose)
        # Plot accuracy graph
        history = self.model_history
        self.plot_model_history(history)

    def test(self, verbose=True):
        '''
    Tests the model by:
    1. Defining a Transmitter/Receiver for the given number of testing bits, modulation and snr
    2. Maps the bits to constellation with AWGN -> This is the Training Data, X
    3. Decodes using tradition decoding -> This is the 'true' value, y
    '''
        self.test_system = Transmitter_Receiver(self.test_size, self.modulation, self.llr_calc)
        self.test_system.send_n_receive(self.test_snr)
        self.test_system.decode()
        N_0_all = np.repeat([self.test_system.N_0],
                            self.test_system.r.shape[1], axis=0).reshape(1, -1)
        if self.input_dim == 3:
            X = np.transpose(np.concatenate((self.test_system.r, N_0_all), axis=0))
        if self.input_dim == 2:
            X = np.transpose(self.test_system.r)
        self.predictions = self.model.predict(X)
        self.decode = np.transpose(0.5 * (-1 * self.predictions / np.abs(self.predictions) + 1))
        self.num_error = np.sum(np.abs(self.test_system.bit_stream - self.decode))
        self.b_error = self.num_error / self.test_system.num_bits_send
        self.conventional_error = self.test_system.bit_error / self.test_system.num_bits_send
        self.accuracy = 1 - np.average(np.abs(self.predictions.T - self.test_system.llr) / np.abs(self.test_system.llr))
        if verbose:
            print('Conventional Decoder bit error rate is %f' % (self.conventional_error))
            print('LLR Net bit error rate is %f' % (self.b_error))

    def plot_model_history(self, history=None):
        if history is None:
            history = self.model_history
        modulation = self.modulation
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
        ax1.plot(history.history['mse'])
        ax1.plot(history.history['val_mse'])
        ax1.set_title('model MSE')
        ax1.set_ylabel('MSE')
        ax1.set_xlabel('epoch')
        ax1.legend(['train', 'test'], loc='upper left')
        # summarize history for loss
        ax2.plot(history.history['loss'])
        ax2.plot(history.history['val_loss'])
        ax2.set_title('model loss')
        ax2.set_ylabel('loss')
        ax2.set_xlabel('epoch')
        ax2.legend(['train', 'test'], loc='upper left')
        plotname = 'logs/{}/{}-{}_ModelHistory.png'.format(NAME, NAME, modulation)
        plt.savefig(plotname)

#

# %%
testmodel_llrnet = LLR_net('64QAM', 5000, 10000, train_snr = 0, test_snr = 5, activation = 'relu')
print(testmodel_llrnet.model.summary())



########################################################################################################################


def load_model(model_object_path):
  with open(model_object_path, 'rb') as f:
      a =pickle.load(f)
  return a.model

# models = ['LLRnet-BPSK-snr20-exact-20230201_17150205_BPSK_model.pkl',
# 'LLRnet-4QAM-snr20-exact-20230201_17181505_4QAM_model.pkl',
# 'LLRnet-16QAM-snr20-exact-20230201_17200605_16QAM_model.pkl',
# 'LLRnet-64QAM-snr20-exact-20230201_17214605_64QAM_model.pkl']

# models = ['LLRnet-16QAM-snr20-exact-20230201_17200605/LLRnet-16QAM-snr20-exact-20230201_17200605_16QAM_model.pkl',
# 'LLRnet-4QAM-snr20-exact-20230201_17181505/LLRnet-4QAM-snr20-exact-20230201_17181505_4QAM_model.pkl',
# 'LLRnet-64QAM-snr20-exact-20230201_17214605/LLRnet-64QAM-snr20-exact-20230201_17214605_64QAM_model.pkl',
# 'LLRnet-BPSK-snr20-exact-20230201_17150205/LLRnet-BPSK-snr20-exact-20230201_17150205_BPSK_model.pkl']

models = ['/home/sheikh/Github/communications_neural_net/logs/LLRnet-BPSK-snr20-exact-20230201_17150205/LLRnet-BPSK-snr20-exact-20230201_17150205_BPSK_model.pkl',
'/home/sheikh/Github/communications_neural_net/logs/LLRnet-4QAM-snr20-exact-20230201_17181505/LLRnet-4QAM-snr20-exact-20230201_17181505_4QAM_model.pkl',
'/home/sheikh/Github/communications_neural_net/logs/LLRnet-16QAM-snr20-exact-20230201_17200605/LLRnet-16QAM-snr20-exact-20230201_17200605_16QAM_model.pkl',
'/home/sheikh/Github/communications_neural_net/logs/LLRnet-64QAM-snr20-exact-20230201_17214605/LLRnet-64QAM-snr20-exact-20230201_17214605_64QAM_model.pkl'
] 
def select_llrnet(num_bits_per_symbol, models = models):

  if num_bits_per_symbol == 1:
    llrnet = load_model(models[0])
  
  if num_bits_per_symbol == 2:
    llrnet = load_model(models[1])

  if num_bits_per_symbol == 4:
    llrnet = load_model(models[2])

  if num_bits_per_symbol == 6:
    llrnet = load_model(models[3])

  return llrnet


class LLRNet(Layer):
    
    def __init__(self, num_bits_per_symbol  = 2, neuron_size = 8, input_dim = 2, activation = 'relu'):
        super().__init__()
        self.num_bits_per_symbol = num_bits_per_symbol
        self.neuron_size = neuron_size # 8 or 16 or 32
        self.input_dim = input_dim # 2 or 3
        self.activation = activation # 'relu' or 'tanh'

    
    def build(self, input_shape):
        
        # Input Layer and First hidden layer
        # model.add(Dense(self.neuron_size, input_dim=self.input_dim, activation=self.activation))
        # # Extra Hidden Layers
        # for i in range(layer_num):
        #     model.add(Dropout(0.2))
        #     model.add(Dense(self.neuron_size, activation=self.activation))
        # self.model = model
        
        
        # Output Layer
        # self.model.add(Dense(self.training_system.k, activation='linear'))
        # self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['mse'])

        # Input convolution
        self._input_dense = Dense(self.neuron_size, 
                                  input_dim=self.input_dim,
                                  activation=self.activation)

        self._output_dense = Dense(self.num_bits_per_symbol, activation='linear')
        
    def call(self, inputs):
        x_hat, no = inputs
        if self.input_dim == 3:
            # Feeding the noise power in log10 scale helps with the performance
            no = log10(no)
            # Stacking the real and imaginary components of the different antennas along the 'channel' dimension
            y = tf.transpose(y, [0, 2, 3, 1]) # Putting antenna dimension last
            no = insert_dims(no, 3, 1)
            no = tf.tile(no, [1, y.shape[1], y.shape[2], 1])
            # z : [batch size, num ofdm symbols, num subcarriers, 2*num rx antenna + 1]
            z = tf.concat([tf.math.real(y),
                        tf.math.imag(y),
                        no], axis=-1)
         
        # Input Dense
        z = self._input_dense(x_hat)
        # Output Dense
        llr_hat = self._output_dense(z)
        return llr_hat

    
    # return z


class NeuralReceiver(Layer):
    r"""
    Keras layer implementing a residual convolutional neural receiver.
    
    This neural receiver is fed with the post-DFT received samples, forming a resource grid of size num_of_symbols x fft_size, and computes LLRs on the transmitted coded bits.
    These LLRs can then be fed to an outer decoder to reconstruct the information bits.
    
    As the neural receiver is fed with the entire resource grid, including the guard bands and pilots, it also computes LLRs for these resource elements.
    They must be discarded to only keep the LLRs corresponding to the data-carrying resource elements.
    
    Input
    ------
    y : [batch size, num rx antenna, num ofdm symbols, num subcarriers], tf.complex
        Received post-DFT samples.
    
    no : [batch size], tf.float32
        Noise variance. At training, a different noise variance value is sampled for each batch example.
    
    Output
    -------
    : [batch size, num ofdm symbols, num subcarriers, num_bits_per_symbol]
        LLRs on the transmitted bits.
        LLRs computed for resource elements not carrying data (pilots, guard bands...) must be discarded.
    """
    
    def build(self, input_shape):
        
        # Input convolution
        self._input_conv = Conv2D(filters=num_conv_channels,
                                  kernel_size=[3,3],
                                  padding='same',
                                  activation=None)
        # Residual blocks
        self._res_block_1 = ResidualBlock()
        self._res_block_2 = ResidualBlock()
        self._res_block_3 = ResidualBlock()
        self._res_block_4 = ResidualBlock()
        # Output conv
        self._output_conv = Conv2D(filters=num_bits_per_symbol,
                                   kernel_size=[3,3],
                                   padding='same',
                                   activation=None)
        
    def call(self, inputs):
        y, no = inputs
        
        # Feeding the noise power in log10 scale helps with the performance
        no = log10(no)
        
        # Stacking the real and imaginary components of the different antennas along the 'channel' dimension
        y = tf.transpose(y, [0, 2, 3, 1]) # Putting antenna dimension last
        no = insert_dims(no, 3, 1)
        no = tf.tile(no, [1, y.shape[1], y.shape[2], 1])
        # z : [batch size, num ofdm symbols, num subcarriers, 2*num rx antenna + 1]
        z = tf.concat([tf.math.real(y),
                       tf.math.imag(y),
                       no], axis=-1)
        # Input conv
        z = self._input_conv(z)
        # Residual blocks
        z = self._res_block_1(z)
        z = self._res_block_2(z)
        z = self._res_block_3(z)
        z = self._res_block_4(z)
        # Output conv
        z = self._output_conv(z)
        
        return z


# In[28]:


# ## Transmitter
# binary_source = BinarySource()
# scrambler = IEEE80211Scrambler(seed = 127)
# interleaver = IEEE80211Interleaver(NCBPS = NCBPS, inverse = False)

# encoder = ConvEncoder(rate=k/n, constraint_length=7)


# constellation = generate_ieee8211_constellation(mu  = num_bits_per_symbol)

# mapper = Mapper(constellation_type = "custom", constellation = constellation)
# rg_mapper = ResourceGridMapper(resource_grid)

# ## Channel
# cdl = CDL(cdl_model, delay_spread, carrier_frequency,
#           ut_antenna, bs_array, "uplink", min_speed=speed)
# channel = OFDMChannel(cdl, resource_grid, normalize_channel=True, return_channel=True)

# ## Receiver
# deinterleaver = IEEE80211Interleaver(NCBPS = NCBPS, inverse = True)

# decoder = ViterbiDecoder(encoder)
# descrambler = IEEE80211Scrambler(seed = 127)
# neural_receiver = NeuralReceiver()
# rg_demapper = ResourceGridDemapper(resource_grid, stream_manager) # Used to extract data-carrying resource elements


# In[29]:


# batch_size = 64
# ebno_db = tf.fill([batch_size], 5.0)
# no = ebnodb2no(ebno_db, num_bits_per_symbol, coderate)


# ## Transmitter
# # Generate codewords


# b = binary_source([batch_size, 1, 1, k])
# print("b shape: ", b.shape)
# # Scramble input bits
# b_scr = scrambler(b)
# print("b_scr shape: ", b_scr.shape)
# # Encode FEC input bits
# c_wo_inter = encoder(b_scr)
# print("c_wo_inter shape: ", c_wo_inter.shape)
# # Interleave encoded bits
# c_inter = interleaver(c_wo_inter)
# print("c shape: ", c_inter.shape)
# # Map bits to QAM symbols
# x = mapper(c_inter)
# print("x shape: ", x.shape)
# # Map the QAM symbols to a resource grid
# x_rg = rg_mapper(x)
# print("x_rg shape: ", x_rg.shape)

# ######################################
# ## Channel
# # A batch of new channel realizations is sampled and applied at every inference
# no_ = expand_to_rank(no, tf.rank(x_rg))
# y,_ = channel([x_rg, no_])
# print("y shape: ", y.shape)

# ######################################
# ## Receiver       
# # The neural receover computes LLRs from the frequency domain received symbols and N0
# y = tf.squeeze(y, axis=1)
# llr = neural_receiver([y, no])
# print("llr shape: ", llr.shape)
# # Reshape the input to fit what the resource grid demapper is expected
# llr = insert_dims(llr, 2, 1)
# # Extract data-carrying resource elements. The other LLRs are discarded
# llr = rg_demapper(llr)
# llr = tf.reshape(llr, [batch_size, 1, 1, n])
# print("Post RG-demapper LLRs: ", llr.shape)


# # In[30]:


# bce = tf.nn.sigmoid_cross_entropy_with_logits(c_inter, llr)
# bce = tf.reduce_mean(bce)
# rate = tf.constant(1.0, tf.float32) - bce/tf.math.log(2.)
# print(f"Rate: {rate:.2E} bit")


# # In[34]:
def llrnet_predict_llr(llrnet,x_hat):

  # llr_hat = llrnet.predict(x_hat)
#   print(x_hat.shape)
  x_hat_siso = tf.squeeze(tf.squeeze(x_hat,1),1)
  x_hat_new = tf.stack([tf.math.real(x_hat_siso), tf.math.imag(x_hat_siso)], -1)
  llr_hat = llrnet(tf.reshape(x_hat_new, [-1,2]))
  return llr_hat

class E2ESystem(Model):
    r"""
    Keras model that implements the end-to-end systems.
    
    As the three considered end-to-end systems (perfect CSI baseline, LS estimation baseline, and neural receiver) share most of
    the link components (transmitter, channel model, outer code...), they are implemented using the same Keras model.

    When instantiating the Keras model, the parameter ``system`` is used to specify the system to setup,
    and the parameter ``training`` is used to specified if the system is instantiated to be trained or to be evaluated.
    The ``training`` parameter is only relevant when the neural 
    
    At each call of this model:
    * A batch of codewords is randomly sampled, modulated, and mapped to resource grids to form the channel inputs
    * A batch of channel realizations is randomly sampled and applied to the channel inputs
    * The receiver is executed on the post-DFT received samples to compute LLRs on the coded bits.
      Which receiver is executed (baseline with perfect CSI knowledge, baseline with LS estimation, or neural receiver) depends
      on the specified ``system`` parameter.
    * If not training, the outer decoder is applied to reconstruct the information bits
    * If training, the BMD rate is estimated over the batch from the LLRs and the transmitted bits
    
    Parameters
    -----------
    system : str
        Specify the receiver to use. Should be one of 'baseline-perfect-csi', 'baseline-ls-estimation' or 'neural-receiver'
    
    training : bool
        Set to `True` if the system is instantiated to be trained. Set to `False` otherwise. Defaults to `False`.
        If the system is instantiated to be trained, the outer encoder and decoder are not instantiated as they are not required for training.
        This significantly reduces the computational complexity of training.
        If training, the bit-metric decoding (BMD) rate is computed from the transmitted bits and the LLRs. The BMD rate is known to be
        an achievable information rate for BICM systems, and therefore training of the neural receiver aims at maximizing this rate.
    
    Input
    ------
    batch_size : int
        Batch size
    
    no : scalar or [batch_size], tf.float
        Noise variance.
        At training, a different noise variance should be sampled for each batch example.
    
    Output
    -------
    If ``training`` is set to `True`, then the output is a single scalar, which is an estimation of the BMD rate computed over the batch. It
    should be used as objective for training.
    If ``training`` is set to `False`, the transmitted information bits and their reconstruction on the receiver side are returned to
    compute the block/bit error rate. 
    """
    
    def __init__(self, system, training=False):
        super().__init__()
        self._system = system
        self._training = training
    
        ######################################
        ## Transmitter
        self._binary_source = BinarySource()
        self._scrambler = IEEE80211Scrambler(seed = 127, batch_size = training_batch_size)
        self._interleaver = IEEE80211Interleaver(NCBPS = NCBPS, inverse = False)
        self._num_bits_per_symbol = num_bits_per_symbol 
        # To reduce the computational complexity of training, the outer code is not used when training,
        # as it is not required
        if not training:
            self._encoder = ConvEncoder(rate=k/n, constraint_length=7)# ConvEncoder(k, n, 7)

        self._constellation = generate_ieee8211_constellation(mu  = self._num_bits_per_symbol)
        
        self._mapper = Mapper(constellation_type = "custom", constellation = self._constellation) #Mapper("qam", num_bits_per_symbol)
        self._rg_mapper = ResourceGridMapper(resource_grid)
        
        ######################################
        ## Channel
        # A 3GPP CDL channel model is used
        cdl = CDL(cdl_model, delay_spread, carrier_frequency,
                  ut_antenna, bs_array, "uplink", min_speed=speed)
        self._channel = OFDMChannel(cdl, resource_grid, normalize_channel=True, return_channel=True)
        
        ######################################
        ## Receiver
        # Four options for the receiver depending on the value of `system`
        if "baseline" in system:
            if system == 'baseline-perfect-csi': # Perfect CSI
                self._removed_null_subc = RemoveNulledSubcarriers(resource_grid)
            elif system == 'baseline-ls-estimation': # LS estimation
                self._ls_est = LSChannelEstimator(resource_grid, interpolation_type="nn")
            # Components required by both baselines
            self._lmmse_equ = LMMSEEqualizer(resource_grid, stream_manager )
            self._demapper = Demapper(demapping_method = "app",
                                      constellation_type =  "custom",
                                      constellation = constellation )   #Demapper("app", "qam", num_bits_per_symbol)
            self._descrambler = IEEE80211Scrambler(seed = 127, batch_size = training_batch_size)
            self._deinterleaver = IEEE80211Interleaver(NCBPS = NCBPS, inverse = True)
        elif "llrnet" in system:
            
            self._llrnet  = LLRNet(num_bits_per_symbol  = self._num_bits_per_symbol, neuron_size = 8, input_dim = 2, activation = 'relu') #select_llrnet(num_bits_per_symbol)
            if system == 'llrnet-perfect-csi': # Perfect CSI
                self._removed_null_subc = RemoveNulledSubcarriers(resource_grid)
            elif system == 'llrnet-ls-estimation': # LS estimation
                self._ls_est = LSChannelEstimator(resource_grid, interpolation_type="nn")
            # Components required by both baselines
            self._lmmse_equ = LMMSEEqualizer(resource_grid, stream_manager)
            self._demapper = Demapper(demapping_method = "app",
                                      constellation_type =  "custom",
                                      constellation = constellation )   #Demapper("app", "qam", num_bits_per_symbol)
            self._descrambler = IEEE80211Scrambler(seed = 127, batch_size = training_batch_size)
            self._deinterleaver = IEEE80211Interleaver(NCBPS = NCBPS, inverse = True)
        
        elif system == "neural-receiver":
            # Neural receiver
            self._neural_receiver = NeuralReceiver()
            self._rg_demapper = ResourceGridDemapper(resource_grid, stream_manager) # Used to extract data-carrying resource elements
            # self._descrambler = IEEE80211Scrambler(seed = 127, batch_size = training_batch_size)
        
        # To reduce the computational complexity of training, the outer code is not used when training,
        # as it is not required
        if not training:

            # self._descrambler = IEEE80211Scrambler(seed = 127, batch_size = training_batch_size)
            # self._deinterleaver = IEEE80211Interleaver(NCBPS = NCBPS, inverse = True)
            self._decoder = ViterbiDecoder(self._encoder) #LDPC5GDecoder(self._encoder, hard_out=True)
    
    # @tf.function
    def call(self, batch_size, ebno_db):
        
        # If `ebno_db` is a scalar, a tensor with shape [batch size] is created as it is what is expected by some layers
        if len(ebno_db.shape) == 0:
            ebno_db = tf.fill([batch_size], ebno_db)
        
        ######################################
        ## Transmitter
        no = ebnodb2no(ebno_db, num_bits_per_symbol, coderate)
        # Outer coding is only performed if not training
        if self._training:
            b = self._binary_source([batch_size, 1, 1, n])
            # b_scr = self._scrambler(b)
            # c = b_scr
            c = b
            
        else:
            
            b = self._binary_source([batch_size, 1, 1, k])

            # With scrambling and interleaving
            # b_scr = self._scrambler(b)
            # c = self._encoder(b_scr)
            # c = self._interleaver(c)
           
            # Without scrambling and interleaving
            c = self._encoder(b)

        # Modulation
        x = self._mapper(c)
        x_rg = self._rg_mapper(x)
        
        ######################################
        ## Channel
        # A batch of new channel realizations is sampled and applied at every inference
        no_ = expand_to_rank(no, tf.rank(x_rg))
        y,h = self._channel([x_rg, no_])
        
        ######################################
        ## Receiver       
        # Three options for the receiver depending on the value of ``system``
        if "baseline" in self._system:
            if self._system == 'baseline-perfect-csi':
                h_hat = self._removed_null_subc(h) # Extract non-null subcarriers
                err_var = 0.0 # No channel estimation error when perfect CSI knowledge is assumed
            elif self._system == 'baseline-ls-estimation':
                h_hat, err_var = self._ls_est([y, no]) # LS channel estimation with nearest-neighbor
            x_hat, no_eff = self._lmmse_equ([y, h_hat, err_var, no]) # LMMSE equalization
            no_eff_= expand_to_rank(no_eff, tf.rank(x_hat))
            llr = self._demapper([x_hat, no_eff_]) # Demapping
        elif "llrnet" in self._system:
            if self._system == 'llrnet-perfect-csi':
                h_hat = self._removed_null_subc(h) # Extract non-null subcarriers
                err_var = 0.0 # No channel estimation error when perfect CSI knowledge is assumed
            elif self._system == 'llrnet-ls-estimation':
                h_hat, err_var = self._ls_est([y, no]) # LS channel estimation with nearest-neighbor
            
            x_hat, no_eff = self._lmmse_equ([y, h_hat, err_var, no]) # LMMSE equalization
            no_eff_= expand_to_rank(no_eff, tf.rank(x_hat))
            # llr = self._demapper([x_hat, no_eff_]) # Demapping
            
            x_hat_siso = tf.squeeze(tf.squeeze(x_hat,1),1)
            x_hat_siso = tf.stack([tf.math.real(x_hat_siso), tf.math.imag(x_hat_siso)], -1) 
            x_hat_siso= tf.reshape(x_hat_siso, [-1,2])  
            
            llr = self._llrnet([x_hat_siso, no])     #llrnet_predict_llr(self._llrnet,x_hat)  * (-1)
            llr = tf.reshape(llr, [batch_size,1,1,n])
           
        elif self._system == "neural-receiver":
            # The neural receover computes LLRs from the frequency domain received symbols and N0
            y = tf.squeeze(y, axis=1)
            llr = self._neural_receiver([y, no])
            llr = insert_dims(llr, 2, 1) # Reshape the input to fit what the resource grid demapper is expected
            llr = self._rg_demapper(llr) # Extract data-carrying resource elements. The other LLrs are discarded
            llr = tf.reshape(llr, [batch_size, 1, 1, n]) # Reshape the LLRs to fit what the outer decoder is expected

        # Outer coding is not needed if the information rate is returned
        if self._training:
            if "neural-receiver" in self._system:
                # Compute and return BMD rate (in bit), which is known to be an achievable
                # information rate for BICM systems.
                # Training aims at maximizing the BMD rate
                bce = tf.nn.sigmoid_cross_entropy_with_logits(c, llr)
                bce = tf.reduce_mean(bce)
                rate = tf.constant(1.0, tf.float32) - bce/tf.math.log(2.)
                return rate
            elif "llrnet" in self._system:
                
                # Compute and return BMD rate (in bit), which is known to be an achievable
                # information rate for BICM systems.
                # Training aims at maximizing the BMD rate
                bce = tf.nn.sigmoid_cross_entropy_with_logits(c, llr)
                bce = tf.reduce_mean(bce)
                rate = tf.constant(1.0, tf.float32) - bce/tf.math.log(2.)
                return rate
        
        else:
            # Outer decoding


            # llr = self._deinterleaver(llr)
            # b_hat_scr = self._decoder(llr)
            # b_hat = self._descrambler(b_hat_scr)

            b_hat = self._decoder(llr)

            
            return b,b_hat # Ground truth and reconstructed information bits returned for BER/BLER computation

"""# 3.0 SNR EbNo value"""

# SNR range for evaluation and training [dB]
ebno_db_min = -30
ebno_db_max = -20
# Range of SNRs over which the systems are evaluated
ebno_dbs = np.arange(ebno_db_min, # Min SNR for evaluation
                     ebno_db_max, # Max SNR for evaluation
                     1) # Step
# Dictionnary storing the evaluation results
BLER = {}
BER = {}
#########################################################################################################################################################
# """# 3. Baseline"""
# print("baseline-perfect-csi Results ")
# model_baseline_perfect_csi = E2ESystem('baseline-perfect-csi')
# ber,bler = sim_ber(model_baseline_perfect_csi, ebno_dbs, batch_size=64, num_target_block_errors=100, max_mc_iter=100)
# BLER['baseline-perfect-csi'] = bler.numpy(); BER['baseline-perfect-csi'] = ber.numpy()
# print(BLER); print(BER)

# print("baseline-ls-estimation Results")
# model_ls_estimation = E2ESystem('baseline-ls-estimation')
# ber,bler = sim_ber(model_ls_estimation, ebno_dbs, batch_size=64, num_target_block_errors=100, max_mc_iter=100)
# BLER['baseline-ls-estimation'] = bler.numpy(); BER['baseline-ls-estimation'] = ber.numpy()
# print(BLER); print(BER)


# print("llrnet-perfect-csi Results ")
# model_llrnet_perfect_csi = E2ESystem('llrnet-perfect-csi', training = False)
# ber,bler = sim_ber(model_llrnet_perfect_csi, ebno_dbs, batch_size=8, num_target_block_errors=100, max_mc_iter=100)
# BLER['llrnet-perfect-csi'] = bler.numpy(); BER['llrnet-perfect-csi'] = ber.numpy()
# print(BLER); print(BER)

# print("llrnet-ls-estimation Results")
# model_llrnet_ls_estimation = E2ESystem('llrnet-ls-estimation', training = False)
# ber,bler = sim_ber(model_llrnet_ls_estimation, ebno_dbs, batch_size=8, num_target_block_errors=100, max_mc_iter=100)
# BLER['llrnet-ls-estimation'] = bler.numpy(); BER['llrnet-ls-estimation'] = ber.numpy()
# print(BLER); print(BER)





# plt.figure(figsize=(10,6))
# # Baseline - Perfect CSI
# plt.semilogy(ebno_dbs, BLER['baseline-perfect-csi'], 'o-', c=f'C0', label=f'Baseline - Perfect CSI')    
# # Baseline - LS Estimation
# plt.semilogy(ebno_dbs, BLER['baseline-ls-estimation'], 'x--', c=f'C1', label=f'Baseline - LS Estimation')
# plt.xlabel(r"$E_b/N_0$ (dB)")
# plt.ylabel("BLER")
# plt.grid(which="both")
# plt.ylim((1e-4, 1.0))
# plt.legend()
# plt.tight_layout()
# #########################################################################################################################################################
# """# 4. Neural Receiver Training"""

# # In[37]:


# # # The end-to-end system equipped with the neural receiver is instantiated for training.
# # # When called, it therefore returns the estimated BMD rate
# # model = E2ESystem('neural-receiver', training=True)

# # # Sampling a batch of SNRs
# # ebno_db = tf.random.uniform(shape=[], minval=ebno_db_min, maxval=ebno_db_max)
# # # Forward pass
# # with tf.GradientTape() as tape:
# #     rate = model(training_batch_size, ebno_db)
# #     # Tensorflow optimizers only know how to minimize loss function.
# #     # Therefore, a loss function is defined as the additive inverse of the BMD rate
# #     loss = -rate

# # # In[38]:


# # optimizer = tf.keras.optimizers.Adam()

# # # Computing and applying gradients        
# # weights = model.trainable_weights
# # grads = tape.gradient(loss, weights)
# # optimizer.apply_gradients(zip(grads, weights))

# # In[39]:




# model_nn_receiver = E2ESystem('neural-receiver', training=True)

# optimizer = tf.keras.optimizers.Adam()

# for i in range(num_training_iterations):
#     # Sampling a batch of SNRs
#     ebno_db = tf.random.uniform(shape=[], minval=ebno_db_min, maxval=ebno_db_max)
#     # Forward pass
#     with tf.GradientTape() as tape:
#         rate = model_nn_receiver(training_batch_size, ebno_db)
#         # Tensorflow optimizers only know how to minimize loss function.
#         # Therefore, a loss function is defined as the additive inverse of the BMD rate
#         loss = -rate
#     # Computing and applying gradients        
#     weights = model_nn_receiver.trainable_weights
#     grads = tape.gradient(loss, weights)
#     optimizer.apply_gradients(zip(grads, weights))
#     # Periodically printing the progress
#     if i % 5 == 0:
#         print('Iteration {}/{}  Rate: {:.4f} bit'.format(i, num_training_iterations, rate.numpy()), end='\r')

# # Save the weights in a file
# weights = model_nn_receiver.get_weights()
# with open(model_weights_path, 'wb') as f:
#     pickle.dump(weights, f)


# # model_nn_receiver = E2ESystem('neural-receiver')

# # # Run one inference to build the layers and loading the weights
# # model_nn_receiver(1, tf.constant(10.0, tf.float32))
# # with open(model_weights_path, 'rb') as f:
# #     weights = pickle.load(f)
# # model_nn_receiver.set_weights(weights)

# # # Evaluations
# # ber,bler = sim_ber(model_nn_receiver, ebno_dbs, batch_size=64, num_target_block_errors=100, max_mc_iter=100)
# # BLER['neural-receiver'] = bler.numpy(); BER['neural-receiver'] = ber.numpy(); 
# # print(BLER); print(BER)



# plt.figure(figsize=(10,6))
# # Baseline - Perfect CSI
# plt.semilogy(ebno_dbs, BLER['baseline-perfect-csi'], 'o-', c=f'C0', label=f'Baseline - Perfect CSI')    
# # Baseline - LS Estimation
# plt.semilogy(ebno_dbs, BLER['baseline-ls-estimation'], 'x--', c=f'C1', label=f'Baseline - LS Estimation')

# # Neural receiver
# # plt.semilogy(ebno_dbs, BLER['neural-receiver'], 's-.', c=f'C2', label=f'Neural receiver')
# # f = model_weights_path + '.png'
# # plt.savefig(f)
# #
# plt.xlabel(r"$E_b/N_0$ (dB)")
# plt.ylabel("BLER")
# plt.grid(which="both")
# plt.ylim((1e-4, 1.0))
# plt.legend()
# plt.tight_layout()






################LLLRNET TRAIN AND EVAL############################################




model_llrnet_perfect_csi =  E2ESystem('llrnet-perfect-csi', training = True)

optimizer = tf.keras.optimizers.Adam()

for i in range(num_training_iterations):
    # Sampling a batch of SNRs
    ebno_db = tf.random.uniform(shape=[], minval=ebno_db_min, maxval=ebno_db_max)
    # Forward pass
    with tf.GradientTape() as tape:
        rate = model_llrnet_perfect_csi(training_batch_size, ebno_db)
        # Tensorflow optimizers only know how to minimize loss function.
        # Therefore, a loss function is defined as the additive inverse of the BMD rate
        loss = -rate
    # Computing and applying gradients        
    weights = model_llrnet_perfect_csi.trainable_weights
    grads = tape.gradient(loss, weights)
    optimizer.apply_gradients(zip(grads, weights))
    # Periodically printing the progress
    if i % 5 == 0:
        print('Iteration {}/{}  Rate: {:.4f} bit'.format(i, num_training_iterations, rate.numpy()), end='\r')

# Save the weights in a file
weights = model_llrnet_perfect_csi.get_weights()
with open(model_weights_path, 'wb') as f:
    pickle.dump(weights, f)


# # In[40]:


model_llrnet = E2ESystem('llrnet-perfect-csi')

# Run one inference to build the layers and loading the weights
model_llrnet(1, tf.constant(10.0, tf.float32))
with open(model_weights_path, 'rb') as f:
    weights = pickle.load(f)
model_llrnet.set_weights(weights)

# # Evaluations
ber,bler = sim_ber(model_llrnet, ebno_dbs, batch_size=64, num_target_block_errors=100, max_mc_iter=100)
BLER['llrnet-perfect-csi'] = bler.numpy(); BER['llrnet-perfect-csi'] = ber.numpy(); 
print(BLER); print(BER)


# plt.figure(figsize=(10,6))
# # Baseline - Perfect CSI
# plt.semilogy(ebno_dbs, BLER['baseline-perfect-csi'], 'o-', c=f'C0', label=f'Baseline - Perfect CSI')    
# # Baseline - LS Estimation
# plt.semilogy(ebno_dbs, BLER['baseline-ls-estimation'], 'x--', c=f'C1', label=f'Baseline - LS Estimation')












# # Neural receiver
# # plt.semilogy(ebno_dbs, BLER['llrnet-perfect-csi'], 's-.', c=f'C2', label=f'Neural receiver')
# # f = model_weights_path + '.png'
# # plt.savefig(f)
# #
# plt.xlabel(r"$E_b/N_0$ (dB)")
# plt.ylabel("BLER")
# plt.grid(which="both")
# plt.ylim((1e-4, 1.0))
# plt.legend()
# plt.tight_layout()


# # In[42]:


# # pre_computed_results = "{'baseline-perfect-csi': [1.0, 1.0, 1.0, 1.0, 1.0, 0.9916930379746836, 0.5367080479452054, 0.0285078125, 0.0017890625, 0.0006171875, 0.0002265625, 9.375e-05, 2.34375e-05, 7.8125e-06, 1.5625e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'baseline-ls-estimation': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9998022151898734, 0.9199448529411764, 0.25374190938511326, 0.0110234375, 0.002078125, 0.0008359375, 0.0004375, 0.000171875, 9.375e-05, 4.6875e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'neural-receiver': [1.0, 1.0, 1.0, 1.0, 1.0, 0.9984177215189873, 0.7505952380952381, 0.10016025641025642, 0.00740625, 0.0021640625, 0.000984375, 0.0003671875, 0.000203125, 0.0001484375, 3.125e-05, 2.34375e-05, 7.8125e-06, 7.8125e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
# # BLER = eval(pre_computed_results)


# # In[ ]:

# # |BLER

# resource_grid.num_data_symbols

# """## References <a class="anchor" id="References"></a>

# [1] M. Honkala, D. Korpi and J. M. J. Huttunen, "DeepRx: Fully Convolutional Deep Learning Receiver," in IEEE Transactions on Wireless Communications, vol. 20, no. 6, pp. 3925-3940, June 2021, doi: 10.1109/TWC.2021.3054520.

# [2] F. Ait Aoudia and J. Hoydis, "End-to-end Learning for OFDM: From Neural Receivers to Pilotless Communication," in IEEE Transactions on Wireless Communications, doi: 10.1109/TWC.2021.3101364.

# [3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, "Deep Residual Learning for Image Recognition", Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778

# [4] G. Bcherer, "Achievable Rates for Probabilistic Shaping", arXiv:1707.01134, 2017.
# """

# # llr.shape

# # b_scr.shape

# # c_wo_inter.shape

